{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Faster R-CNN training.ipynb",
      "provenance": [],
      "mount_file_id": "1eZM9imgixUaIfLXcpK0y0lxoS3XIB7Zh",
      "authorship_tag": "ABX9TyNu25YKuvCNUZC0AZMb7Sge",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lu-lab/frcnn-all-in-one/blob/main/colab/Faster_R_CNN_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47FdKExYaZY1"
      },
      "source": [
        "\n",
        "# Faster R-CNN training\n",
        "---\n",
        "This notebook will allow you to train a custom object detector using Google's GPU resources. Enable this by going to Runtime -> Change runtime type and select \"GPU\" from the dropdown menu. This will speed your training time up substantially, but note that Google has a limit on how much of this GPU resource you can use. If you use the GPU resource heavily, you may have to subscribe to a paid plan. In this notebook, we will be fine-tuning the Faster-RCNN network (specifically, the Faster R-CNN Inception ResNet V2) starting from a model pre-trained on the COCO 2017 image set. This pre-trained model is provided in the Tensorflow 2 Model Zoo [here](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md)). \n",
        "\n",
        "\n",
        "###Step 00: Copy this notebook\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "First off, **save a copy of this notebook to your own Google Drive!** This is partially to protect your data and partially so you can save any changes you may want to make to this notebook.\n",
        "\n",
        "**Note**: Following this notebook will require several GB of space in your Google Drive, in addition to whatever space you may need for your annotated image data, or any data you may want to perform inferences on. \n",
        "\n",
        "\n",
        "###Step 0: Annotate your data\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Before you can train your model, you need a **train and test set of annotations** for your images, and the annotations should be converted into the tensorflow-friendly tfrecord format. If you do not have these already, you can use our binder here [![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/lu-lab/frcnn-all-in-one/HEAD) to do that! **Once you have finished working through the binder and have downloaded the bounding_boxes.csv and label_map.pbtxt files generated** to your computer, return here. If you used the binder to convert a movie to images for labelling, **download the images folder as well and upload it to the folder this code is in.**\n",
        "\n",
        "###Step 1: Connect to your Google Drive\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Now that you're back, we'll mount Google Drive, get files we need from the GitHub repository to run this notebook, and make a few new folders that we will put data in. Make sure your Runtime type is set to GPU (Runtime -> Change runtime type and select \"GPU\" from the dropdown menu), and run the following cell. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5UD9VAxApKO"
      },
      "source": [
        "# First we need to mount Google drive and gather some dependencies...\n",
        "from google.colab import drive \n",
        "drive.mount('/content/drive')\n",
        "import os\n",
        "\n",
        "# NOTE: Modify this path if needed\n",
        "%cd /content/drive/My\\ Drive/Colab\\ Notebooks/Faster\\ R-CNN/\n",
        "working_dir = os.getcwd()\n",
        "\n",
        "!git clone https://github.com/lu-lab/frcnn-all-in-one.git\n",
        "%cd {working_dir}\n",
        "!cp -a ./frcnn-all-in-one/colab/. .\n",
        "!rm -r ./frcnn-all-in-one\n",
        "!rm Faster_R_CNN_training.ipynb\n",
        "\n",
        "# if any of these folders already exist, they will not be made!\n",
        "!mkdir annotations\n",
        "!mkdir images\n",
        "!mkdir exported-model\n",
        "!mkdir inferencing-results\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9afs9QthBlsX"
      },
      "source": [
        "###Step 2: Install libraries\n",
        "\n",
        "---\n",
        "\n",
        "Next we install all the necessary libraries to train our model. At the end of this step, you'll also have several new folders, including Tensorflow, COCO-trained-model, and cocoapi. The Tensorflow directory includes most of the model development pipeline, the cocoapi allows the pipeline to compute metrics describing how well the model performs as we train it, and the COCO-trained-model is our starting model that we will be fine-tuning.\n",
        "\n",
        "**Note**: If you've already run this notebook once, you do NOT need to re-download the Tensorflow, cocoapi, and COCO-trained-model files, but you DO need to install them and other libraries again if you have stopped the session! If you just need to re-install everything, put a # in front of every line in the code below that has a # at the end of it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t87xb4CyaYNr"
      },
      "source": [
        "# You will need to re-install everything if you've restarted your session\n",
        "%mkdir Tensorflow \n",
        "%cd ./Tensorflow\n",
        "!git clone https://github.com/tensorflow/models.git #\n",
        "%cd {working_dir}\n",
        "!apt-get -qq install -y protobuf-compiler python-pil python-lxml python-tk \n",
        "%cd ./Tensorflow/models/research/\n",
        "!protoc object_detection/protos/*.proto --python_out=.\n",
        "%cd {working_dir}\n",
        "\n",
        "!git clone https://github.com/cocodataset/cocoapi.git #\n",
        "%cd ./cocoapi/PythonAPI\n",
        "!make\n",
        "!cp -r pycocotools /content/drive/My\\ Drive/Colab\\ Notebooks/Faster\\ R-CNN/TensorFlow/models/research/\n",
        "%cd {working_dir}\n",
        "\n",
        "%cd Tensorflow/models/research/\n",
        "!cp object_detection/packages/tf2/setup.py .\n",
        "!python -m pip install .\n",
        "%cd {working_dir}\n",
        "\n",
        "%mkdir COCO-trained-model\n",
        "%cd COCO-trained-model\n",
        "!wget http://download.tensorflow.org/models/object_detection/tf2/20200711/faster_rcnn_inception_resnet_v2_1024x1024_coco17_tpu-8.tar.gz #\n",
        "!tar -xf faster_rcnn_inception_resnet_v2_1024x1024_coco17_tpu-8.tar.gz #\n",
        "!rm faster_rcnn_inception_resnet_v2_1024x1024_coco17_tpu-8.tar.gz #\n",
        "!mv -v faster_rcnn_inception_resnet_v2_1024x1024_coco17_tpu-8/* /content/drive/My\\ Drive/Colab\\ Notebooks/Faster\\ R-CNN/COCO-trained-model/ #\n",
        "!rmdir faster_rcnn_inception_resnet_v2_1024x1024_coco17_tpu-8 #\n",
        "%cd {working_dir}\n",
        "\n",
        "!cp ./Tensorflow/models/research/object_detection/model_main_tf2.py . #\n",
        "!cp ./Tensorflow/models/research/object_detection/exporter_main_v2.py . #\n",
        "\n",
        "!pip install Cython==0.29.21\n",
        "!pip install h5py==2.10.0\n",
        "!pip install opencv-python-headless==4.4.0.44\n",
        "!pip install scipy\n",
        "!pip install tensorflow\n",
        "!pip install tensorboard\n",
        "\n",
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wx3G_i2aFgsh"
      },
      "source": [
        "### Step 3: Generate tfrecord files\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Now we want to use the bounding_boxes.csv and label_map.pbtxt file to generate tfrecords. Put the 'bounding_boxes.csv' file in the 'annotations' folder and the 'label_map.pbtxt' file in the 'training' folder before running the following cell.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "slsul2V_cvUB"
      },
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "sys.path.append(\"./Tensorflow/models/research/object_detection\")\n",
        "from object_detection.utils import label_map_util\n",
        "import generate_tfrecord as gt\n",
        "\n",
        "def write_tf_record(annotations, tfrecord_path, img_path, label_map):\n",
        "    tf_writer = tf.io.TFRecordWriter(tfrecord_path)\n",
        "    for annotation in annotations:\n",
        "        tf_example = gt.create_tf_example(annotation, img_path, label_map)\n",
        "        tf_writer.write(tf_example.SerializeToString())\n",
        "\n",
        "    tf_writer.close()\n",
        "    output_path = os.path.join(os.getcwd(), tfrecord_path)\n",
        "    print('Successfully created the TFRecords: {}'.format(output_path))\n",
        "\n",
        "label_map_path = './training/label_map.pbtxt'\n",
        "csv_filepath = './annotations/bounding_boxes.csv'\n",
        "\n",
        "# and now we'll convert annotations to a tfrecord\n",
        "label_map = label_map_util.get_label_map_dict(label_map_path)\n",
        "all_annotations = pd.read_csv(csv_filepath)\n",
        "img_path = './images'\n",
        "\n",
        "# tfrecord for train annotations\n",
        "tfrecord_train_path = './annotations/train.record'\n",
        "train_annotations = all_annotations[all_annotations['test_or_train'].isin(['train'])]\n",
        "grouped_train_annotations = gt.split(train_annotations, 'filename')\n",
        "write_tf_record(grouped_train_annotations, tfrecord_train_path, img_path, label_map)\n",
        "\n",
        "# tfrecord for test annotations\n",
        "tfrecord_test_path = './annotations/test.record'\n",
        "test_annotations = all_annotations[all_annotations['test_or_train'].isin(['test'])]\n",
        "grouped_test_annotations = gt.split(test_annotations, 'filename')\n",
        "write_tf_record(grouped_test_annotations, tfrecord_test_path, img_path, label_map)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-RRTCy5xeS62"
      },
      "source": [
        "### Step 4: Train the model\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "Now we will fine-tune the model with our own images and classes. Note that if anything is not in it's proper folder when you run this step, it will fail. You will see a lot of warnings as this step starts to run - not to worry, as long as you don't see an error it should be ok! This step will take a long time, even with the GPU (likely at least a few hours). It is up to you when to stop training (you can stop the kernel altogether, or Ctrl+C), and generally a good rule of thumb is to watch for when the total loss starts to plateau. You should be able to monitor this by watching the Tensorboard that will start when you run the following cell. You can also monitor this in the output above the Tensorboard widget once the first 100 training steps are complete. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cTwq00LL2DPt"
      },
      "source": [
        "import tensorflow as tf\n",
        "import datetime\n",
        "\n",
        "%tensorboard --logdir training/train/\n",
        "!python model_main_tf2.py --model_dir=training --pipeline_config_path=training/faster_rcnn.config --alsologtostderr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgvHvLdh4je3"
      },
      "source": [
        "### Step 5: Export model\n",
        "\n",
        "---\n",
        "\n",
        "Once the loss becomes reasonable and you've stopped training, freeze the model and save it to the 'exported-model' folder. You'll see quite a few warnings when you run the following cell, but again no need for concern unless you see an error. Once this is done, double check your 'exported-model' folder. It should now contain a 'checkpoint' and 'saved-model' folder and a 'pipeline.config' file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9wtxkBrb5JZn"
      },
      "source": [
        "!python exporter_main_v2.py \\\n",
        "--input_type image_tensor \\\n",
        "--pipeline_config_path ./training/faster_rcnn.config \\\n",
        "--trained_checkpoint_dir ./training/ \\\n",
        "--output_directory ./exported-model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gk3ioxUqObXR"
      },
      "source": [
        "### **Step 6: Test inferencing**\n",
        "\n",
        "Now our model is frozen, we can use it to inference, or predict the bounding boxes for the classes we trained on with new images. Below are a few different options for how to do this, depending on whether your input data is a list of images, a folder of images, or a movie. \n",
        "\n",
        "** NEEDS TO BE UPDATED FOR TF2**\n",
        "\n",
        "#### **Inferencing for our test images**\n",
        "First, let's see how the model performs on our annotated test images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KCpCxB9YPF7b"
      },
      "source": [
        "import sys\n",
        "import glob\n",
        "import IPython\n",
        "from IPython.display import Image, display\n",
        "import cv2\n",
        "from inference_code import inferencing_tools\n",
        "\n",
        "\n",
        "path_to_frozen_graph = './exported-model/frozen_inference_graph.pb'\n",
        "# we've already defined this above, but just in case\n",
        "path_to_labels = './training/label_map.pbtxt'\n",
        "save_to_hdf = True\n",
        "save_path = './inferencing-results'\n",
        "h5_file = os.path.join(save_path, 'test_detections.h5')\n",
        "\n",
        "# every inference you make with this 'cnn' object will be saved in the same h5 file\n",
        "cnn = inferencing_tools.CNN(path_to_frozen_graph, path_to_labels, save_to_hdf, h5_file)\n",
        "\n",
        "# take a look at qualitatively how well the model performs on our test images\n",
        "test_image_paths = test_images\n",
        "\n",
        "for path in test_image_paths:\n",
        "    image_np = cv2.imread(path)\n",
        "    #inferencing happens in this call\n",
        "    worm_boxes, egg_boxes = cnn.get_eggs_and_worms(image_np, path)\n",
        "\n",
        "# Now that we have all the detections, label them on the test data and visualize the detections on each test image.\n",
        "inferencing_tools.label_all_detections_from_h5(h5_file, test_image_paths, save_path)\n",
        "for image_name in glob.glob('./inferencing-results/*.JPG'): #assuming JPG\n",
        "    display(Image(filename=image_name))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bd_JveUNVUTC"
      },
      "source": [
        "#### **Inferencing from a folder of images**\n",
        "This is very similar to the example above. Here we detect all worms and eggs from .jpg images in a folder, saves them to an h5 file, and then overlay the detections on top of the original images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-QX1UzEVf-U"
      },
      "source": [
        "path_to_frozen_graph = './exported-model/frozen_inference_graph.pb'\n",
        "# we've already defined this above, but just in case\n",
        "path_to_labels = './training/label_map.pbtxt'\n",
        "save_to_hdf = True\n",
        "save_path = './inferencing'\n",
        "h5_file = os.path.join(save_path, 'folder_detections.h5')\n",
        "\n",
        "# every inference you make with this 'cnn' object will be saved in the same h5 file\n",
        "cnn = inferencing_tools.CNN(path_to_frozen_graph, path_to_labels, save_to_hdf, h5_file)\n",
        "\n",
        "# take a look at qualitatively how well the model performs on our test images\n",
        "image_dir = './images'\n",
        "\n",
        "image_paths = [f for f in os.listdir(image_dir) if os.path.isfile(os.path.join(image_dir, f))\n",
        "                             and f.endswith('.jpg')]\n",
        "\n",
        "for path in image_paths:\n",
        "    image_np = cv2.imread(path)\n",
        "    #inferencing happens in this call - you can directly use the boxes this returns if you wish. They are also saved in the h5 file\n",
        "    worm_boxes, egg_boxes = cnn.get_eggs_and_worms(image_np, path)\n",
        "\n",
        "# Now that we have all the detections, label them on the test data and visualize the detections on each test image.\n",
        "inferencing_tools.label_all_detections_from_h5(h5_file, image_paths, save_path)\n",
        "for image_name in glob.glob('./inferencing/*.JPG'): #assuming JPG\n",
        "    display(Image(filename=image_name))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awzXEbVNVukB"
      },
      "source": [
        "#### **Inferencing from a movie**\n",
        "Here's an example that detects both eggs and worms for each frame in a video, saves the detections to an h5 file, and then overlays all detections on top of the original images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjisdN9YV4Ez"
      },
      "source": [
        "path_to_frozen_graph = './exported-model/frozen_inference_graph.pb'\n",
        "# we've already defined this above, but just in case\n",
        "path_to_labels = './training/label_map.pbtxt'\n",
        "save_to_hdf = True\n",
        "save_path = './inferencing'\n",
        "h5_file = os.path.join(save_path, 'movie_detections.h5')\n",
        "\n",
        "# every inference you make with this 'cnn' object will be saved in the same h5 file\n",
        "cnn = inferencing_tools.CNN(path_to_frozen_graph, path_to_labels, save_to_hdf, h5_file)\n",
        "\n",
        "# detect and visualize detections from movie\n",
        "movie_path = ''\n",
        "save_file = ''\n",
        "\n",
        "vid = cv2.VideoCapture(source_data)\n",
        "idx = 1\n",
        "while vid.isOpened():\n",
        "    ret, image = vid.read()\n",
        "    if ret:\n",
        "        #inferencing happens in this call\n",
        "        worm_boxes, egg_boxes = cnn.get_eggs_and_worms(image, idx)\n",
        "    else:\n",
        "        break\n",
        "    print(\"Processing frame no %s\" % i)\n",
        "    idx += 1\n",
        "vid.release()\n",
        "\n",
        "inferencing_tools.label_all_detections_from_h5(h5_file, movie_path, save_file)\n",
        "\n",
        "# if the input of the inferencing is a video, the output will be a video, otherwise it will be an image\n",
        "Video(save_file)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
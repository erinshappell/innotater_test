{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Faster R-CNN training.ipynb",
      "provenance": [],
      "mount_file_id": "1eZM9imgixUaIfLXcpK0y0lxoS3XIB7Zh",
      "authorship_tag": "ABX9TyNVeGTPyu2oV9OK/VkSClkQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lu-lab/frcnn-all-in-one/blob/main/colab/Faster_R_CNN_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47FdKExYaZY1"
      },
      "source": [
        "\n",
        "# Faster R-CNN training\n",
        "---\n",
        "This notebook will allow you to train a custom object detector using Google's GPU resources. Enable this by going to Runtime -> Change runtime type and select \"GPU\" from the dropdown menu. This will speed your training time up substantially, but note that Google has a limit on how much of this GPU resource you can use. If you use the GPU resource heavily, you may have to subscribe to a paid plan. In this notebook, we will be fine-tuning the Faster-RCNN network (specifically, the Faster R-CNN Inception ResNet V2) starting from a model pre-trained on the COCO 2017 image set. This pre-trained model is provided in the Tensorflow 2 Model Zoo [here](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md)). \n",
        "\n",
        "\n",
        "###Step 00: Copy this folder\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "First off, you won't be able to edit this notebook directly, so **save a copy of this notebook to your own Google Drive!**\n",
        "\n",
        "**Note**: Following this notebook will require several GB of space in your Google Drive, in addition to whatever space you may need for your annotated image data, or any data you may want to perform inferences on. \n",
        "\n",
        "\n",
        "###Step 0: Annotate your data\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Before you can train your model, you need a **train and test set of annotations** for your images, and the annotations should be converted into the tensorflow-friendly tfrecord format. If you do not have these already, you can use our binder here [![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/lu-lab/frcnn-all-in-one/HEAD) to do that! **Once you have finished working through the binder and have downloaded the bounding_boxes.csv and label_map.pbtxt files generated** to your computer, return here. If you used the binder to convert a movie to images for labelling, **download the images folder as well and upload it to the folder this code is in.**\n",
        "\n",
        "###Step 1: Connect to your Google Drive\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Now that you're back, make sure your Runtime type is set to GPU (Runtime -> Change runtime type and select \"GPU\" from the dropdown menu), let's make a few new folders in our current directory by running the following cell. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q5UD9VAxApKO",
        "outputId": "beedbdc0-375c-4c11-ba5f-cea2fff01849"
      },
      "source": [
        "# First we need to mount Google drive and gather some dependencies...\n",
        "from google.colab import drive \n",
        "drive.mount('/content/drive')\n",
        "import os\n",
        "\n",
        "# NOTE: Modify this path if needed\n",
        "%cd /content/drive/My\\ Drive/Colab\\ Notebooks/Faster\\ R-CNN/\n",
        "working_dir = os.getcwd()\n",
        "\n",
        "# if any of these folders already exist, they will not be made!\n",
        "!mkdir annotations\n",
        "!mkdir images\n",
        "!mkdir exported-model\n",
        "!mkdir inferencing-results\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/My Drive/Colab Notebooks/Faster R-CNN\n",
            "mkdir: cannot create directory ‘annotations’: File exists\n",
            "mkdir: cannot create directory ‘images’: File exists\n",
            "mkdir: cannot create directory ‘exported-model’: File exists\n",
            "mkdir: cannot create directory ‘inferencing-results’: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9afs9QthBlsX"
      },
      "source": [
        "###Step 2: Install libraries\n",
        "\n",
        "---\n",
        "\n",
        "Next we install all the necessary libraries to train our model. At the end of this step, you'll also have several new folders, including Tensorflow, COCO-trained-model, and cocoapi. The Tensorflow directory includes most of the model development pipeline, the cocoapi allows the pipeline to compute metrics describing how well the model performs as we train it, and the COCO-trained-model is our starting model that we will be fine-tuning.\n",
        "\n",
        "**Note**: If you've already run this notebook once, you do NOT need to re-download the Tensorflow, cocoapi, and COCO-trained-model files, but you DO need to install them and other libraries again if you have stopped the session! If you just need to re-install everything, put a # in front of every line in the code below that has a # at the end of it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t87xb4CyaYNr"
      },
      "source": [
        "# You will need to re-install everything if you've restarted your session\n",
        "%mkdir Tensorflow \n",
        "%cd ./Tensorflow\n",
        "# !git clone https://github.com/tensorflow/models.git #\n",
        "%cd {working_dir}\n",
        "!apt-get -qq install -y protobuf-compiler python-pil python-lxml python-tk \n",
        "%cd ./Tensorflow/models/research/\n",
        "!protoc object_detection/protos/*.proto --python_out=.\n",
        "%cd {working_dir}\n",
        "\n",
        "# !git clone https://github.com/cocodataset/cocoapi.git #\n",
        "%cd ./cocoapi/PythonAPI\n",
        "!make\n",
        "!cp -r pycocotools /content/drive/My\\ Drive/Colab\\ Notebooks/Faster\\ R-CNN/TensorFlow/models/research/\n",
        "%cd {working_dir}\n",
        "\n",
        "%cd Tensorflow/models/research/\n",
        "!cp object_detection/packages/tf2/setup.py .\n",
        "!python -m pip install .\n",
        "%cd {working_dir}\n",
        "\n",
        "%mkdir COCO-trained-model\n",
        "%cd COCO-trained-model\n",
        "# !wget http://download.tensorflow.org/models/object_detection/tf2/20200711/faster_rcnn_inception_resnet_v2_1024x1024_coco17_tpu-8.tar.gz #\n",
        "# !tar -xf faster_rcnn_inception_resnet_v2_1024x1024_coco17_tpu-8.tar.gz #\n",
        "# !rm faster_rcnn_inception_resnet_v2_1024x1024_coco17_tpu-8.tar.gz #\n",
        "# !mv -v faster_rcnn_inception_resnet_v2_1024x1024_coco17_tpu-8/* /content/drive/My\\ Drive/Colab\\ Notebooks/Faster\\ R-CNN/COCO-trained-model/ #\n",
        "# !rmdir faster_rcnn_inception_resnet_v2_1024x1024_coco17_tpu-8 #\n",
        "%cd {working_dir}\n",
        "\n",
        "# !cp ./Tensorflow/models/research/object_detection/model_main_tf2.py . #\n",
        "# !cp ./Tensorflow/models/research/object_detection/exporter_main_v2.py . #\n",
        "\n",
        "!pip install Cython==0.29.21\n",
        "!pip install h5py==2.10.0\n",
        "!pip install opencv-python-headless==4.4.0.44\n",
        "!pip install scipy\n",
        "!pip install tensorflow\n",
        "!pip install tensorboard\n",
        "\n",
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wx3G_i2aFgsh"
      },
      "source": [
        "### Step 3: Generate tfrecord files\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Now we want to use the bounding_boxes.csv and label_map.pbtxt file to generate tfrecords. Put the 'bounding_boxes.csv' file in the 'annotations' folder and the 'label_map.pbtxt' file in the 'training' folder before running the following cell.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "slsul2V_cvUB",
        "outputId": "b6faef67-24a8-49c2-a7de-8aa9d126c758"
      },
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "sys.path.append(\"./Tensorflow/models/research/object_detection\")\n",
        "from object_detection.utils import label_map_util\n",
        "import generate_tfrecord as gt\n",
        "\n",
        "def write_tf_record(annotations, tfrecord_path, img_path, label_map):\n",
        "    tf_writer = tf.io.TFRecordWriter(tfrecord_path)\n",
        "    for annotation in annotations:\n",
        "        tf_example = gt.create_tf_example(annotation, img_path, label_map)\n",
        "        tf_writer.write(tf_example.SerializeToString())\n",
        "\n",
        "    tf_writer.close()\n",
        "    output_path = os.path.join(os.getcwd(), tfrecord_path)\n",
        "    print('Successfully created the TFRecords: {}'.format(output_path))\n",
        "\n",
        "label_map_path = './training/label_map.pbtxt'\n",
        "csv_filepath = './annotations/bounding_boxes.csv'\n",
        "\n",
        "# and now we'll convert annotations to a tfrecord\n",
        "label_map = label_map_util.get_label_map_dict(label_map_path)\n",
        "all_annotations = pd.read_csv(csv_filepath)\n",
        "img_path = './images'\n",
        "\n",
        "# tfrecord for train annotations\n",
        "tfrecord_train_path = './annotations/train.record'\n",
        "train_annotations = all_annotations[all_annotations['test_or_train'].isin(['train'])]\n",
        "grouped_train_annotations = gt.split(train_annotations, 'filename')\n",
        "write_tf_record(grouped_train_annotations, tfrecord_train_path, img_path, label_map)\n",
        "\n",
        "# tfrecord for test annotations\n",
        "tfrecord_test_path = './annotations/test.record'\n",
        "test_annotations = all_annotations[all_annotations['test_or_train'].isin(['test'])]\n",
        "grouped_test_annotations = gt.split(test_annotations, 'filename')\n",
        "write_tf_record(grouped_test_annotations, tfrecord_test_path, img_path, label_map)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Successfully created the TFRecords: /content/drive/My Drive/Colab Notebooks/Faster R-CNN/./annotations/train.record\n",
            "Successfully created the TFRecords: /content/drive/My Drive/Colab Notebooks/Faster R-CNN/./annotations/test.record\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-RRTCy5xeS62"
      },
      "source": [
        "### Step 4: Train the model\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "Now we will fine-tune the model with our own images and classes. Note that if anything is not in it's proper folder when you run this step, it will fail. You will see a lot of warnings as this step starts to run - not to worry, as long as you don't see an error it should be ok! This step will take a long time, even with the GPU (likely at least a few hours). It is up to you when to stop training (you can stop the kernel altogether, or Ctrl+C), and generally a good rule of thumb is to watch for when the total loss starts to plateau. You should be able to monitor this by watching the Tensorboard that will start when you run the following cell. You can also monitor this in the output above the Tensorboard widget once the first 100 training steps are complete. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cTwq00LL2DPt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "580b1ed9-49e5-494a-b643-9061ac1a7280"
      },
      "source": [
        "import tensorflow as tf\n",
        "import datetime\n",
        "\n",
        "%tensorboard --logdir training/train/\n",
        "!python model_main_tf2.py --model_dir=training --pipeline_config_path=training/faster_rcnn.config --alsologtostderr"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"model_main_tf2.py\", line 113, in <module>\n",
            "    tf.compat.v1.app.run()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/platform/app.py\", line 40, in run\n",
            "    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/absl/app.py\", line 303, in run\n",
            "    _run_main(main, args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/absl/app.py\", line 251, in _run_main\n",
            "    sys.exit(main(argv))\n",
            "  File \"model_main_tf2.py\", line 110, in main\n",
            "    record_summaries=FLAGS.record_summaries)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/object_detection/model_lib_v2.py\", line 667, in train_loop\n",
            "    loss = _dist_train_step(train_input_iter)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\", line 828, in __call__\n",
            "    result = self._call(*args, **kwds)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\", line 855, in _call\n",
            "    return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\", line 2943, in __call__\n",
            "    filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\", line 1919, in _call_flat\n",
            "    ctx, args, cancellation_manager=cancellation_manager))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\", line 560, in call\n",
            "    ctx=ctx)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\", line 60, in quick_execute\n",
            "    inputs, attrs, num_outputs)\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgvHvLdh4je3"
      },
      "source": [
        "### Step 5: Export model\n",
        "\n",
        "---\n",
        "\n",
        "Once the loss becomes reasonable and you've stopped training, freeze the model and save it to the 'exported-model' folder. You'll see quite a few warnings when you run the following cell, but again no need for concern unless you see an error. Once this is done, double check your 'exported-model' folder. It should now contain a 'checkpoint' and 'saved-model' folder and a 'pipeline.config' file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9wtxkBrb5JZn",
        "outputId": "bd3e01b0-08f9-4203-f846-07bf0f93e602"
      },
      "source": [
        "!python exporter_main_v2.py \\\n",
        "--input_type image_tensor \\\n",
        "--pipeline_config_path ./training/faster_rcnn.config \\\n",
        "--trained_checkpoint_dir ./training/ \\\n",
        "--output_directory ./exported-model"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-04-20 16:40:26.682677: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "2021-04-20 16:40:31.895720: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
            "2021-04-20 16:40:31.936408: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
            "2021-04-20 16:40:32.016685: E tensorflow/stream_executor/cuda/cuda_driver.cc:328] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "2021-04-20 16:40:32.016772: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (acd3afe05462): /proc/driver/nvidia/version does not exist\n",
            "2021-04-20 16:40:32.017655: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/object_detection/exporter_lib_v2.py:106: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with back_prop=False is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n",
            "Instead of:\n",
            "results = tf.map_fn(fn, elems, back_prop=False)\n",
            "Use:\n",
            "results = tf.nest.map_structure(tf.stop_gradient, tf.map_fn(fn, elems))\n",
            "W0420 16:40:33.521652 140383446521728 deprecation.py:604] From /usr/local/lib/python3.7/dist-packages/object_detection/exporter_lib_v2.py:106: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with back_prop=False is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n",
            "Instead of:\n",
            "results = tf.map_fn(fn, elems, back_prop=False)\n",
            "Use:\n",
            "results = tf.nest.map_structure(tf.stop_gradient, tf.map_fn(fn, elems))\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0420 16:40:57.211076 140383446521728 convolutional_keras_box_predictor.py:154] depth of additional conv before box predictor: 0\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:201: calling crop_and_resize_v1 (from tensorflow.python.ops.image_ops_impl) with box_ind is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "box_ind is deprecated, use box_indices instead\n",
            "W0420 16:41:04.927993 140383446521728 deprecation.py:537] From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:201: calling crop_and_resize_v1 (from tensorflow.python.ops.image_ops_impl) with box_ind is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "box_ind is deprecated, use box_indices instead\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/object_detection/utils/model_util.py:57: Tensor.experimental_ref (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use ref() instead.\n",
            "W0420 16:41:05.560802 140383446521728 deprecation.py:339] From /usr/local/lib/python3.7/dist-packages/object_detection/utils/model_util.py:57: Tensor.experimental_ref (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use ref() instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:201: batch_gather (from tensorflow.python.ops.array_ops) is deprecated and will be removed after 2017-10-25.\n",
            "Instructions for updating:\n",
            "`tf.batch_gather` is deprecated, please use `tf.gather` with `batch_dims=-1` instead.\n",
            "W0420 16:41:18.141758 140383446521728 deprecation.py:339] From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:201: batch_gather (from tensorflow.python.ops.array_ops) is deprecated and will be removed after 2017-10-25.\n",
            "Instructions for updating:\n",
            "`tf.batch_gather` is deprecated, please use `tf.gather` with `batch_dims=-1` instead.\n",
            "2021-04-20 16:41:19.499142: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
            "2021-04-20 16:41:19.523025: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2299995000 Hz\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.meta_architectures.faster_rcnn_meta_arch.FasterRCNNMetaArch object at 0x7fad2ffa7b90>, because it is not built.\n",
            "W0420 16:41:34.410123 140383446521728 save_impl.py:78] Skipping full serialization of Keras layer <object_detection.meta_architectures.faster_rcnn_meta_arch.FasterRCNNMetaArch object at 0x7fad2ffa7b90>, because it is not built.\n",
            "2021-04-20 16:42:18.990272: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
            "W0420 16:43:19.433020 140383446521728 save.py:241] Found untraced functions such as FirstStageBoxPredictor_layer_call_and_return_conditional_losses, FirstStageBoxPredictor_layer_call_fn, mask_rcnn_keras_box_predictor_layer_call_and_return_conditional_losses, mask_rcnn_keras_box_predictor_layer_call_fn, FirstStageBoxPredictor_layer_call_fn while saving (showing 5 of 70). These functions will not be directly callable after loading.\n",
            "W0420 16:43:22.848367 140383446521728 save.py:241] Found untraced functions such as FirstStageBoxPredictor_layer_call_and_return_conditional_losses, FirstStageBoxPredictor_layer_call_fn, mask_rcnn_keras_box_predictor_layer_call_and_return_conditional_losses, mask_rcnn_keras_box_predictor_layer_call_fn, FirstStageBoxPredictor_layer_call_fn while saving (showing 5 of 70). These functions will not be directly callable after loading.\n",
            "INFO:tensorflow:Assets written to: ./exported-model/saved_model/assets\n",
            "I0420 16:43:38.480496 140383446521728 builder_impl.py:775] Assets written to: ./exported-model/saved_model/assets\n",
            "INFO:tensorflow:Writing pipeline config file to ./exported-model/pipeline.config\n",
            "I0420 16:43:40.806655 140383446521728 config_util.py:254] Writing pipeline config file to ./exported-model/pipeline.config\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gk3ioxUqObXR"
      },
      "source": [
        "### **Step 6: Test inferencing**\n",
        "\n",
        "Now our model is frozen, we can use it to inference, or predict the bounding boxes for the classes we trained on with new images. Below are a few different options for how to do this, depending on whether your input data is a list of images, a folder of images, or a movie. \n",
        "\n",
        "** NEEDS TO BE UPDATED FOR TF2**\n",
        "\n",
        "#### **Inferencing for our test images**\n",
        "First, let's see how the model performs on our annotated test images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "id": "KCpCxB9YPF7b",
        "outputId": "612d6bb0-bf4b-4ae3-f22f-0e4cd7a63782"
      },
      "source": [
        "import sys\n",
        "import glob\n",
        "import IPython\n",
        "from IPython.display import Image, display\n",
        "import cv2\n",
        "from inference_code import inferencing_tools\n",
        "\n",
        "\n",
        "path_to_frozen_graph = './exported-model/frozen_inference_graph.pb'\n",
        "# we've already defined this above, but just in case\n",
        "path_to_labels = './training/label_map.pbtxt'\n",
        "save_to_hdf = True\n",
        "save_path = './inferencing-results'\n",
        "h5_file = os.path.join(save_path, 'test_detections.h5')\n",
        "\n",
        "# every inference you make with this 'cnn' object will be saved in the same h5 file\n",
        "cnn = inferencing_tools.CNN(path_to_frozen_graph, path_to_labels, save_to_hdf, h5_file)\n",
        "\n",
        "# take a look at qualitatively how well the model performs on our test images\n",
        "test_image_paths = test_images\n",
        "\n",
        "for path in test_image_paths:\n",
        "    image_np = cv2.imread(path)\n",
        "    #inferencing happens in this call\n",
        "    worm_boxes, egg_boxes = cnn.get_eggs_and_worms(image_np, path)\n",
        "\n",
        "# Now that we have all the detections, label them on the test data and visualize the detections on each test image.\n",
        "inferencing_tools.label_all_detections_from_h5(h5_file, test_image_paths, save_path)\n",
        "for image_name in glob.glob('./inferencing-results/*.JPG'): #assuming JPG\n",
        "    display(Image(filename=image_name))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-ea646b0906b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# every inference you make with this 'cnn' object will be saved in the same h5 file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mcnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minferencing_tools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_to_frozen_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath_to_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_to_hdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# take a look at qualitatively how well the model performs on our test images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/Colab Notebooks/Faster R-CNN/inference_code/inferencing_tools.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, graph_path, labelmap_path, save_processed_images, save_dir)\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh5_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'processed'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'data.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcwd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcategory_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'CNN: tensorflow model loaded'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/Colab Notebooks/Faster R-CNN/inference_code/inferencing_tools.py\u001b[0m in \u001b[0;36mload_graph\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mod_graph_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraphDef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfid\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m                 \u001b[0mserialized_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m                 \u001b[0mod_graph_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParseFromString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mserialized_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_graph_def\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mod_graph_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/lib/io/file_io.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    115\u001b[0m       \u001b[0mstring\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstring\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mregular\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \"\"\"\n\u001b[0;32m--> 117\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_preread_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m       \u001b[0mlength\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/lib/io/file_io.py\u001b[0m in \u001b[0;36m_preread_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     78\u001b[0m                                            \"File isn't open for reading\")\n\u001b[1;32m     79\u001b[0m       self._read_buf = _pywrap_file_io.BufferedInputStream(\n\u001b[0;32m---> 80\u001b[0;31m           compat.path_to_str(self.__name), 1024 * 512)\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_prewrite_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotFoundError\u001b[0m: ./exported-model/frozen_inference_graph.pb; No such file or directory"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bd_JveUNVUTC"
      },
      "source": [
        "#### **Inferencing from a folder of images**\n",
        "This is very similar to the example above. Here we detect all worms and eggs from .jpg images in a folder, saves them to an h5 file, and then overlay the detections on top of the original images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-QX1UzEVf-U"
      },
      "source": [
        "path_to_frozen_graph = './exported-model/frozen_inference_graph.pb'\n",
        "# we've already defined this above, but just in case\n",
        "path_to_labels = './training/label_map.pbtxt'\n",
        "save_to_hdf = True\n",
        "save_path = './inferencing'\n",
        "h5_file = os.path.join(save_path, 'folder_detections.h5')\n",
        "\n",
        "# every inference you make with this 'cnn' object will be saved in the same h5 file\n",
        "cnn = inferencing_tools.CNN(path_to_frozen_graph, path_to_labels, save_to_hdf, h5_file)\n",
        "\n",
        "# take a look at qualitatively how well the model performs on our test images\n",
        "image_dir = './images'\n",
        "\n",
        "image_paths = [f for f in os.listdir(image_dir) if os.path.isfile(os.path.join(image_dir, f))\n",
        "                             and f.endswith('.jpg')]\n",
        "\n",
        "for path in image_paths:\n",
        "    image_np = cv2.imread(path)\n",
        "    #inferencing happens in this call - you can directly use the boxes this returns if you wish. They are also saved in the h5 file\n",
        "    worm_boxes, egg_boxes = cnn.get_eggs_and_worms(image_np, path)\n",
        "\n",
        "# Now that we have all the detections, label them on the test data and visualize the detections on each test image.\n",
        "inferencing_tools.label_all_detections_from_h5(h5_file, image_paths, save_path)\n",
        "for image_name in glob.glob('./inferencing/*.JPG'): #assuming JPG\n",
        "    display(Image(filename=image_name))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awzXEbVNVukB"
      },
      "source": [
        "#### **Inferencing from a movie**\n",
        "Here's an example that detects both eggs and worms for each frame in a video, saves the detections to an h5 file, and then overlays all detections on top of the original images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjisdN9YV4Ez"
      },
      "source": [
        "path_to_frozen_graph = './exported-model/frozen_inference_graph.pb'\n",
        "# we've already defined this above, but just in case\n",
        "path_to_labels = './training/label_map.pbtxt'\n",
        "save_to_hdf = True\n",
        "save_path = './inferencing'\n",
        "h5_file = os.path.join(save_path, 'movie_detections.h5')\n",
        "\n",
        "# every inference you make with this 'cnn' object will be saved in the same h5 file\n",
        "cnn = inferencing_tools.CNN(path_to_frozen_graph, path_to_labels, save_to_hdf, h5_file)\n",
        "\n",
        "# detect and visualize detections from movie\n",
        "movie_path = ''\n",
        "save_file = ''\n",
        "\n",
        "vid = cv2.VideoCapture(source_data)\n",
        "idx = 1\n",
        "while vid.isOpened():\n",
        "    ret, image = vid.read()\n",
        "    if ret:\n",
        "        #inferencing happens in this call\n",
        "        worm_boxes, egg_boxes = cnn.get_eggs_and_worms(image, idx)\n",
        "    else:\n",
        "        break\n",
        "    print(\"Processing frame no %s\" % i)\n",
        "    idx += 1\n",
        "vid.release()\n",
        "\n",
        "inferencing_tools.label_all_detections_from_h5(h5_file, movie_path, save_file)\n",
        "\n",
        "# if the input of the inferencing is a video, the output will be a video, otherwise it will be an image\n",
        "Video(save_file)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
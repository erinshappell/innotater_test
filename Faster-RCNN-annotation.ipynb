{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Faster R-CNN all-in-one "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3242,
     "status": "ok",
     "timestamp": 1617656991968,
     "user": {
      "displayName": "Kathleen Bates",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhUseX9sBA-nekh3hN1QsuIKVXq0OAj5O1n98Su3A=s64",
      "userId": "02528001053972876322"
     },
     "user_tz": 240
    },
    "id": "LELov4JKDEhB",
    "outputId": "02f05a8b-40b7-4c7b-8767-ec1b6464b979"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A subdirectory or file training already exists.\n",
      "A subdirectory or file exported-model already exists.\n",
      "A subdirectory or file annotations already exists.\n"
     ]
    }
   ],
   "source": [
    "# !apt-get install protobuf-compiler python-pil python-lxml python-tk\n",
    "# !git clone https://github.com/tensorflow/models.git\n",
    "# !protoc object_detection/protos/*.proto --python_out=.\n",
    "\n",
    "# !pip install jupyter_innotater\n",
    "# !apt-get install unzip\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import itertools\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "%load_ext tensorboard\n",
    "import tensorflow as tf\n",
    "sys.path.append(\"./models/research\")\n",
    "from object_detection.utils import label_map_util\n",
    "\n",
    "import generate_tfrecord as gt\n",
    "\n",
    "# make some folders to organize our files\n",
    "\n",
    "# ! mkdir pre-trained-model\n",
    "# %cd ./pre-trained-model\n",
    "# !wget --no-check-certificate https://ndownloader.figshare.com/files/26260501\n",
    "# %cd ..\n",
    "! mkdir training\n",
    "! mkdir exported-model\n",
    "! mkdir annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2C5-BVhy1qfZ"
   },
   "source": [
    "## Annotating frames from a movie\n",
    "If you wish to annotate frames from a movie, first convert them into images and save them in a new folder called 'images' in the same directory as this. **If your data is already saved as images, skip this step and don't run the following cell.**\n",
    "You should specify the filepath of your movie you want to annotate as the \n",
    "\n",
    "```\n",
    "video_path \n",
    "```\n",
    "\n",
    "variable below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 69032,
     "status": "ok",
     "timestamp": 1617657082548,
     "user": {
      "displayName": "Kathleen Bates",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhUseX9sBA-nekh3hN1QsuIKVXq0OAj5O1n98Su3A=s64",
      "userId": "02528001053972876322"
     },
     "user_tz": 240
    },
    "id": "OL9N-HGw05Nc",
    "outputId": "8a2ad262-7d31-4741-99c9-d6882054bd0c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "! mkdir images\n",
    "video_path = \"worm_pump02.mp4\"\n",
    "vid = cv2.VideoCapture(video_path)\n",
    "i = 1\n",
    "while vid.isOpened():\n",
    "    ret, image_np = vid.read()\n",
    "    if ret:\n",
    "        frame_name = 'img' + str(i) + '.jpg'\n",
    "        cv2.imwrite(os.path.join(\"images\", frame_name), image_np)\n",
    "    else:\n",
    "        break\n",
    "    print(\"Processing frame no %s\" % i)\n",
    "    i += 1\n",
    "vid.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J3bDo7Ju47t9"
   },
   "source": [
    "## Annotating images\n",
    "Once your data is in image form, we will use Innotator to annotate it. If you did not run the code above to convert a movie to a set of images, make a folder called \"images\" in the same folder as this and upload your images there. **Make sure the images are named as imgxx.jpg, where xx is a number**\n",
    "\n",
    "Depending on your needs, you may not need to annotate all that many images. Here we only demonstrate fine-tuning a pre-existing model, and while we used between ~100 - 1000 annotations for our models, if you want very high accuracy, you may need to annotate more images. \n",
    "\n",
    "Once you run the code cell below, you will see an interface for annotation appear. Draw boxes around the types of objects you wish to annotate ('classes'). By default, the classes we use below are those in our paper, 'worm' and 'egg'. Advance to the next image by clicking ‘Next’ or pressing ‘n’ on the keyboard (provided the annotation tool has focus)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17,
     "referenced_widgets": [
      "c687f13ad34d42d39020ddba5c25ee1d"
     ]
    },
    "executionInfo": {
     "elapsed": 732,
     "status": "ok",
     "timestamp": 1617658448094,
     "user": {
      "displayName": "Kathleen Bates",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhUseX9sBA-nekh3hN1QsuIKVXq0OAj5O1n98Su3A=s64",
      "userId": "02528001053972876322"
     },
     "user_tz": 240
    },
    "id": "NkRh0gqk5MlZ",
    "outputId": "8be32c49-0024-465e-8555-0683746ee59b"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcee734413a24eaaae3ee63e0baf80f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Innotater(children=(HBox(children=(VBox(children=(ImagePad(value=b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from jupyter_innotater import *\n",
    "import glob\n",
    "\n",
    "img_path = './images'\n",
    "worm_image_files = glob.glob('./images/*.JPG')\n",
    "worm_image_files.sort(key=os.path.getmtime)\n",
    "\n",
    "# 'Repeats' is the maximum number of objects you expect to annotate in each image\n",
    "repeats = 7\n",
    "# Feel free to modify the classes, or types of objects, you want to identify\n",
    "classes = ['worm', 'egg']\n",
    "\n",
    "# Binary flag to indicate an image should be excluded from dataset\n",
    "targets_exclude = np.zeros((len(worm_image_files), 1), dtype='int') \n",
    "\n",
    "# set up arrays to load annotation info into\n",
    "targets_classes = np.zeros((len(worm_image_files), len(classes)*repeats), dtype='int')\n",
    "targets_bboxes = np.zeros((len(worm_image_files), len(classes)*repeats, 4), dtype='int') # (xmin,ymin,w,h) for each animal\n",
    "\n",
    "Innotater(\n",
    "    [\n",
    "        ImageInnotation(worm_image_files, path='./images'), # Display the image itself\n",
    "        TextInnotation(worm_image_files, multiline=False) # Display the image filename\n",
    "    ],\n",
    "    [\n",
    "        BinaryClassInnotation(targets_exclude, name='Exclude'), # Checkbox\n",
    "        RepeatInnotation(\n",
    "            (BoundingBoxInnotation, targets_bboxes), # Individual animal bounding box\n",
    "            (MultiClassInnotation, targets_classes,\n",
    "                {'name':'object', 'classes':classes, 'dropdown':True}), # Per-annotation dropdown\n",
    "            max_repeats=len(classes)*repeats, min_repeats=1\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert annotations to tfrecord format\n",
    "Tensorflow uses the tfrecord format as input to our model. To convert to this format, first we'll prepare the annotations, save them as a csv file, and build a label map from our list of classes. We will split up annotations now so that the precision and accuracy of the model can be tested later. We choose images at random and choose a 90/ 10 train/test split for the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# reshape data so that the structure is flat\n",
    "flat_bboxes = np.reshape(targets_bboxes, (-1, 4))\n",
    "flat_classes = np.reshape(targets_classes, (-1,))\n",
    "flat_classes = [classes[x] for x in flat_classes]\n",
    "\n",
    "# Grab widths and heights for each image \n",
    "widths = np.zeros((len(worm_image_files),), dtype='int')\n",
    "heights = np.zeros((len(worm_image_files),), dtype='int')\n",
    "for i, im_file in enumerate(worm_image_files):\n",
    "    im = cv2.imread(os.path.join('./images', im_file))\n",
    "    heights[i], widths[i], d = im.shape\n",
    "            \n",
    "# it can be helpful to have annotations stored in a generic csv as well, so we'll prepare annotations and do that here.\n",
    "data = {'filename': np.repeat(worm_image_files, len(classes)*repeats), 'xmin': flat_bboxes[:,0], 'ymin': flat_bboxes[:,1], \n",
    "        'xmax': np.add(flat_bboxes[:,0], flat_bboxes[:,2]), 'ymax': np.add(flat_bboxes[:,1], flat_bboxes[:,3]), \n",
    "        'width': np.repeat(widths, len(classes)*repeats), 'height':np.repeat(heights, len(classes)*repeats),\n",
    "        'exclude': np.repeat(np.reshape(targets_exclude, (-1,)), len(classes)*repeats), 'class': flat_classes}\n",
    "df = pd.DataFrame(data)\n",
    "# screen out any un-annotated frames or any images marked by the annotator as 'Exclude'\n",
    "df_annotated = df[(df['exclude'] == 0) & (df['xmin'] != df['xmax'])]\n",
    "\n",
    "# if you want to change the train/ test split, you can decrease number of training images (which will increase the testing images) by decreasing this. It must be between 0 and 1\n",
    "train_split = 0.9 \n",
    "annotated_images = df_annotated['filename'].unique()\n",
    "num_train_images = int(train_split*len(annotated_images))\n",
    "train_images = random.sample(list(annotated_images), num_train_images)\n",
    "test_images = list(set(annotated_images).difference(set(train_images)))\n",
    "test_idx = df_annotated['filename'].isin(test_images)\n",
    "test_or_train = ['test' if test_i else 'train' for test_i in test_idx]\n",
    "df_annotated.insert(9, 'test_or_train', test_or_train)\n",
    "\n",
    "# save csv to annotations directory\n",
    "csv_filepath = './annotations/bounding_boxes.csv'\n",
    "df_annotated.to_csv(csv_filepath, index=False)\n",
    "\n",
    "# next, make a label file so we know how to map the names of classes to a number. 'worm' will map to 1, 'egg' will map to 2, and so on\n",
    "label_map_path = './training/label_map.pbtxt'\n",
    "gt.write_label_map(classes, label_map_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now build tfrecord files for train and test data, saving them in the 'annotations' folder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created the TFRecords: C:\\Users\\kebel\\Dropbox (GaTech)\\rcnn_extras\\code dump\\./annotations/train.record\n",
      "Successfully created the TFRecords: C:\\Users\\kebel\\Dropbox (GaTech)\\rcnn_extras\\code dump\\./annotations/test.record\n"
     ]
    }
   ],
   "source": [
    "def write_tf_record(annotations, tfrecord_path, img_path, label_map):\n",
    "    tf_writer = tf.io.TFRecordWriter(tfrecord_path)\n",
    "    for annotation in annotations:\n",
    "        tf_example = gt.create_tf_example(annotation, img_path, label_map)\n",
    "        tf_writer.write(tf_example.SerializeToString())\n",
    "\n",
    "    tf_writer.close()\n",
    "    output_path = os.path.join(os.getcwd(), tfrecord_path)\n",
    "    print('Successfully created the TFRecords: {}'.format(output_path))\n",
    "\n",
    "\n",
    "# and now we'll convert annotations to a tfrecord\n",
    "label_map = label_map_util.get_label_map_dict(label_map_path)\n",
    "all_annotations = pd.read_csv(csv_filepath)\n",
    "img_path = './images'\n",
    "\n",
    "# tfrecord for train annotations\n",
    "tfrecord_train_path = './annotations/train.record'\n",
    "train_annotations = all_annotations[all_annotations['test_or_train'].isin(['train'])]\n",
    "grouped_train_annotations = gt.split(train_annotations, 'filename')\n",
    "write_tf_record(grouped_train_annotations, tfrecord_train_path, img_path, label_map)\n",
    "\n",
    "# tfrecord for test annotations\n",
    "tfrecord_test_path = './annotations/test.record'\n",
    "test_annotations = all_annotations[all_annotations['test_or_train'].isin(['test'])]\n",
    "grouped_test_annotations = gt.split(test_annotations, 'filename')\n",
    "write_tf_record(grouped_test_annotations, tfrecord_test_path, img_path, label_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ERROR: Timed out waiting for TensorBoard to start. It may still be running as pid 27420."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def makeWindowsCmdPath(path):\n",
    "    return '\\\"' + str(path) + '\\\"'\n",
    "\n",
    "log_dir = r\"C:\\Users\\kebel\\Dropbox%20(GaTech)\\rcnn_extras\\code%20dump\\training\\train\"\n",
    "%tensorboard --logdir {makeWindowsCmdPath(log_dir)}\n",
    "\n",
    "!python model_main_tf2.py --model_dir=training --pipeline_config_path=training/faster_rcnn.config --alsologtostderr "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python object_detection/export_inference_graph.py \\\n",
    "--input_type=image_tensor \\\n",
    "--pipeline_config_path=training/faster_rcnn.config \\\n",
    "--trained_checkpoint_prefix=/training/model.ckpt-1691 \\\n",
    "--output_directory=exported-model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test model\n",
    "First, take a look at how well the model performs on our test data we decided on above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Video, Image, display\n",
    "import glob\n",
    "from inference_code import inferencing_tools\n",
    "\n",
    "! mkdir inferencing\n",
    "\n",
    "path_to_frozen_graph = './exported-model/frozen_inference_graph.pb'\n",
    "# we've already defined this above, but just in case\n",
    "path_to_labels = './training/label_map.pbtxt'\n",
    "save_to_hdf = True\n",
    "save_path = './inferencing'\n",
    "h5_file = os.path.join(save_path, 'test_detections.h5')\n",
    "\n",
    "# every inference you make with this 'cnn' object will be saved in the same h5 file\n",
    "cnn = inferencing_tools.CNN(path_to_frozen_graph, path_to_labels, save_to_hdf, h5_file)\n",
    "\n",
    "# take a look at qualitatively how well the model performs on our test images\n",
    "test_image_paths = test_images\n",
    "\n",
    "for path in test_image_paths:\n",
    "    image_np = cv2.imread(path)\n",
    "    #inferencing happens in this call\n",
    "    worm_boxes, egg_boxes = cnn.get_eggs_and_worms(image_np, path)\n",
    "\n",
    "# Now that we have all the detections, label them on the test data and visualize the detections on each test image.\n",
    "inferencing_tools.label_all_detections_from_h5(h5_file, test_image_paths, save_path)\n",
    "for image_name in glob.glob('./inferencing/*.JPG'): #assuming JPG\n",
    "    display(Image(filename=image_name))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To inference on other data, we will use similar methods to that above. Here's an example that detects all worms and eggs from .jpg images in a folder, saves them to an h5 file, and then overlays the detections on top of the original images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_frozen_graph = './exported-model/frozen_inference_graph.pb'\n",
    "# we've already defined this above, but just in case\n",
    "path_to_labels = './training/label_map.pbtxt'\n",
    "save_to_hdf = True\n",
    "save_path = './inferencing'\n",
    "h5_file = os.path.join(save_path, 'folder_detections.h5')\n",
    "\n",
    "# every inference you make with this 'cnn' object will be saved in the same h5 file\n",
    "cnn = inferencing_tools.CNN(path_to_frozen_graph, path_to_labels, save_to_hdf, h5_file)\n",
    "\n",
    "# take a look at qualitatively how well the model performs on our test images\n",
    "image_dir = './images'\n",
    "\n",
    "image_paths = [f for f in os.listdir(image_dir) if os.path.isfile(os.path.join(image_dir, f))\n",
    "                             and f.endswith('.jpg')]\n",
    "\n",
    "for path in image_paths:\n",
    "    image_np = cv2.imread(path)\n",
    "    #inferencing happens in this call - you can directly use the boxes this returns if you wish. They are also saved in the h5 file\n",
    "    worm_boxes, egg_boxes = cnn.get_eggs_and_worms(image_np, path)\n",
    "\n",
    "# Now that we have all the detections, label them on the test data and visualize the detections on each test image.\n",
    "inferencing_tools.label_all_detections_from_h5(h5_file, image_paths, save_path)\n",
    "for image_name in glob.glob('./inferencing/*.JPG'): #assuming JPG\n",
    "    display(Image(filename=image_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an example that detects both eggs and worms for each frame in a video, saves the detections to an h5 file, and then overlays just the detection of the top-scoring worm on top of the original images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_frozen_graph = './exported-model/frozen_inference_graph.pb'\n",
    "# we've already defined this above, but just in case\n",
    "path_to_labels = './training/label_map.pbtxt'\n",
    "save_to_hdf = True\n",
    "save_path = './inferencing'\n",
    "h5_file = os.path.join(save_path, 'movie_detections.h5')\n",
    "\n",
    "# every inference you make with this 'cnn' object will be saved in the same h5 file\n",
    "cnn = inferencing_tools.CNN(path_to_frozen_graph, path_to_labels, save_to_hdf, h5_file)\n",
    "\n",
    "# detect and visualize detections from movie\n",
    "movie_path = ''\n",
    "save_file = ''\n",
    "\n",
    "vid = cv2.VideoCapture(source_data)\n",
    "idx = 1\n",
    "while vid.isOpened():\n",
    "    ret, image = vid.read()\n",
    "    if ret:\n",
    "        #inferencing happens in this call\n",
    "        worm_boxes, egg_boxes = cnn.get_eggs_and_worms(image, idx)\n",
    "    else:\n",
    "        break\n",
    "    print(\"Processing frame no %s\" % i)\n",
    "    idx += 1\n",
    "vid.release()\n",
    "\n",
    "inferencing_tools.label_all_detections_from_h5(h5_file, movie_path, save_file)\n",
    "\n",
    "# if the input of the inferencing is a video, the output will be a video, otherwise it will be an image\n",
    "Video(save_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMNJV5K0MBQ8t21+IDuilDa",
   "collapsed_sections": [],
   "name": "test_innotation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "c687f13ad34d42d39020ddba5c25ee1d": {
     "model_module": "jupyter-innotater",
     "model_name": "InnotaterModel",
     "state": {
      "_dom_classes": [
       "innotater-base"
      ],
      "_model_module": "jupyter-innotater",
      "_model_module_version": "~0.2.2",
      "_model_name": "InnotaterModel",
      "_view_count": null,
      "_view_module": "jupyter-innotater",
      "_view_module_version": "~0.2.2",
      "_view_name": "InnotaterView",
      "box_style": "",
      "children": [
       "IPY_MODEL_bca4be32bcf84dc3bad36014cec87753",
       "IPY_MODEL_c49643bff7bc4ba2a177e837d4616a87"
      ],
      "index": 0,
      "is_dirty": false,
      "keyboard_shortcuts": true,
      "layout": "IPY_MODEL_37fef569811440b490fefa86a1354e37"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

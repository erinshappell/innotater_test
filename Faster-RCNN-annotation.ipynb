{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Faster R-CNN annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3242,
     "status": "ok",
     "timestamp": 1617656991968,
     "user": {
      "displayName": "Kathleen Bates",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhUseX9sBA-nekh3hN1QsuIKVXq0OAj5O1n98Su3A=s64",
      "userId": "02528001053972876322"
     },
     "user_tz": 240
    },
    "id": "LELov4JKDEhB",
    "outputId": "02f05a8b-40b7-4c7b-8767-ec1b6464b979"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import itertools\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# make a folder for everything we will need to download (except the images that will be annotated)\n",
    "!mkdir download\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2C5-BVhy1qfZ"
   },
   "source": [
    "## Annotating frames from a movie\n",
    "If you wish to annotate frames from a movie, first convert them into images and save them in a new folder called 'images' in the same directory as this. **If your data is already saved as images, skip this step and don't run the following cell.**\n",
    "You should specify the filepath of your movie you want to annotate as the \n",
    "\n",
    "```\n",
    "video_path \n",
    "```\n",
    "\n",
    "variable below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 69032,
     "status": "ok",
     "timestamp": 1617657082548,
     "user": {
      "displayName": "Kathleen Bates",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhUseX9sBA-nekh3hN1QsuIKVXq0OAj5O1n98Su3A=s64",
      "userId": "02528001053972876322"
     },
     "user_tz": 240
    },
    "id": "OL9N-HGw05Nc",
    "outputId": "8a2ad262-7d31-4741-99c9-d6882054bd0c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "! mkdir images\n",
    "video_path = \"worm_pump02.mp4\"\n",
    "vid = cv2.VideoCapture(video_path)\n",
    "i = 1\n",
    "while vid.isOpened():\n",
    "    ret, image_np = vid.read()\n",
    "    if ret:\n",
    "        frame_name = 'img' + str(i) + '.jpg'\n",
    "        cv2.imwrite(os.path.join(\"images\", frame_name), image_np)\n",
    "    else:\n",
    "        break\n",
    "    print(\"Processing frame no %s\" % i)\n",
    "    i += 1\n",
    "vid.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J3bDo7Ju47t9"
   },
   "source": [
    "## Annotating images\n",
    "Once your data is in image form, we will use Innotater to annotate it. If you did not run the code above to convert a movie to a set of images, make a folder called \"images\" in the same folder as this and upload your images there. **To upload your images**, first go to the Jupyter home page or click on the Jupyter icon in the upper left of the notebook. You'll see the directory of the binder, and you can make a new folder and upload images to it using the 'New' dropdown menu and the 'Upload' button in the right corner of the home page. **Make sure the images are named as imgxx.jpg, where xx is a number**. For other image extensions, e.g. .png, you can replace JPG with png in the code below, but note that we have previously had problems with tensorflow not recognizing .png files during training.\n",
    "\n",
    "Depending on your needs, you may not need to annotate all that many images. Here we only demonstrate fine-tuning a pre-existing model, and while we used between ~100 - 1000 annotations for our models, if you want very high accuracy, you may need to annotate more images.\n",
    "\n",
    "Once you run the code cell below, you will see an interface for annotation appear. Draw boxes around the types of objects you wish to annotate ('classes'). By default, the classes we use below are those in our paper, 'worm' and 'egg'. Advance to the next image by clicking ‘Next’ or pressing ‘n’ on the keyboard (provided the annotation tool has focus)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17,
     "referenced_widgets": [
      "c687f13ad34d42d39020ddba5c25ee1d"
     ]
    },
    "executionInfo": {
     "elapsed": 732,
     "status": "ok",
     "timestamp": 1617658448094,
     "user": {
      "displayName": "Kathleen Bates",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhUseX9sBA-nekh3hN1QsuIKVXq0OAj5O1n98Su3A=s64",
      "userId": "02528001053972876322"
     },
     "user_tz": 240
    },
    "id": "NkRh0gqk5MlZ",
    "outputId": "8be32c49-0024-465e-8555-0683746ee59b"
   },
   "outputs": [],
   "source": [
    "from jupyter_innotater import *\n",
    "import glob\n",
    "\n",
    "img_path = './images'\n",
    "worm_image_files = glob.glob('./images/*.JPG')\n",
    "worm_image_files.sort(key=os.path.getmtime)\n",
    "worm_image_files = [os.path.split(file)[1] for file in worm_image_files]\n",
    "worm_image_files = worm_image_files.reverse()\n",
    "\n",
    "# 'Repeats' is the maximum number of objects you expect to annotate in each image\n",
    "repeats = 7\n",
    "# Feel free to modify the classes, or types of objects, you want to identify\n",
    "classes = ['worm', 'egg']\n",
    "\n",
    "# Binary flag to indicate an image should be excluded from dataset\n",
    "targets_exclude = np.zeros((len(worm_image_files), 1), dtype='int') \n",
    "\n",
    "# set up arrays to load annotation info into\n",
    "targets_classes = np.zeros((len(worm_image_files), len(classes)*repeats), dtype='int')\n",
    "targets_bboxes = np.zeros((len(worm_image_files), len(classes)*repeats, 4), dtype='int') # (xmin,ymin,w,h) for each animal\n",
    "\n",
    "Innotater(\n",
    "    [\n",
    "        ImageInnotation(worm_image_files, path='./images'), # Display the image itself\n",
    "        TextInnotation(worm_image_files, multiline=False) # Display the image filename\n",
    "    ],\n",
    "    [\n",
    "        BinaryClassInnotation(targets_exclude, name='Exclude'), # Checkbox\n",
    "        RepeatInnotation(\n",
    "            (BoundingBoxInnotation, targets_bboxes), # Individual animal bounding box\n",
    "            (MultiClassInnotation, targets_classes,\n",
    "                {'name':'object', 'classes':classes, 'dropdown':True}), # Per-annotation dropdown\n",
    "            max_repeats=len(classes)*repeats, min_repeats=1\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save annotations\n",
    "Here we'll prepare the annotations, save them as a csv file, and build a label map from our list of classes. We will split up annotations now so that the precision and accuracy of the model can be tested later. We choose images at random and choose a 90/ 10 train/test split for the data. Note that tensorflow uses the tfrecord format as input to our model. We won't convert to tfrecord format here, as it requires installing Tensorflow, but you can follow our Google CoLab notebook to continue the training process for a Faster R-CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# reshape data so that the structure is flat\n",
    "flat_bboxes = np.reshape(targets_bboxes, (-1, 4))\n",
    "flat_classes = np.reshape(targets_classes, (-1,))\n",
    "flat_classes = [classes[x] for x in flat_classes]\n",
    "\n",
    "# Grab widths and heights for each image \n",
    "widths = np.zeros((len(worm_image_files),), dtype='int')\n",
    "heights = np.zeros((len(worm_image_files),), dtype='int')\n",
    "for i, im_file in enumerate(worm_image_files):\n",
    "    im = cv2.imread(os.path.join('./images', im_file))\n",
    "    heights[i], widths[i], d = im.shape\n",
    "            \n",
    "# it can be helpful to have annotations stored in a generic csv as well, so we'll prepare annotations and do that here.\n",
    "data = {'filename': np.repeat(worm_image_files, len(classes)*repeats), 'xmin': flat_bboxes[:,0], 'ymin': flat_bboxes[:,1], \n",
    "        'xmax': np.add(flat_bboxes[:,0], flat_bboxes[:,2]), 'ymax': np.add(flat_bboxes[:,1], flat_bboxes[:,3]), \n",
    "        'width': np.repeat(widths, len(classes)*repeats), 'height':np.repeat(heights, len(classes)*repeats),\n",
    "        'exclude': np.repeat(np.reshape(targets_exclude, (-1,)), len(classes)*repeats), 'class': flat_classes}\n",
    "df = pd.DataFrame(data)\n",
    "# screen out any un-annotated frames or any images marked by the annotator as 'Exclude'\n",
    "df_annotated = df[(df['exclude'] == 0) & (df['xmin'] != df['xmax'])]\n",
    "\n",
    "# if you want to change the train/ test split, you can decrease number of training images (which will increase the testing images) by decreasing this. It must be between 0 and 1\n",
    "train_split = 0.9 \n",
    "annotated_images = df_annotated['filename'].unique()\n",
    "num_train_images = int(train_split*len(annotated_images))\n",
    "train_images = random.sample(list(annotated_images), num_train_images)\n",
    "test_images = list(set(annotated_images).difference(set(train_images)))\n",
    "test_idx = df_annotated['filename'].isin(test_images)\n",
    "test_or_train = ['test' if test_i else 'train' for test_i in test_idx]\n",
    "df_annotated.insert(9, 'test_or_train', test_or_train)\n",
    "\n",
    "# save csv to annotations directory\n",
    "csv_filepath = './download/bounding_boxes.csv'\n",
    "df_annotated.to_csv(csv_filepath, index=False)\n",
    "\n",
    "# next, make a label file so we know how to map the names of classes to a number. 'worm' will map to 1, 'egg' will map to 2, and so on\n",
    "def write_label_map(classes, label_map_path):\n",
    "    s = ' '\n",
    "    for ID, name in enumerate(classes):\n",
    "        out = ''\n",
    "        out += 'item' + s + '{\\n'\n",
    "        out += '\\t' + 'id:' + s + (str(ID+1)) + '\\n'\n",
    "        out += '\\t' + 'name:' + s + '\\'' + name + '\\'' + '\\n'\n",
    "        out += '}\\n\\n'\n",
    "        with open(label_map_path, 'a') as f:\n",
    "            f.write(out)\n",
    "        \n",
    "label_map_path = './download/label_map.pbtxt'\n",
    "write_label_map(classes, label_map_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download your files\n",
    "Once your label_map.pbtxt and bounding_boxes.csv files have been successfully generated, **download the 'download' folder to your local computer**. If you converted a video to images in order to annotate, you also need to download the 'images' folder!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMNJV5K0MBQ8t21+IDuilDa",
   "collapsed_sections": [],
   "name": "test_innotation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "c687f13ad34d42d39020ddba5c25ee1d": {
     "model_module": "jupyter-innotater",
     "model_name": "InnotaterModel",
     "state": {
      "_dom_classes": [
       "innotater-base"
      ],
      "_model_module": "jupyter-innotater",
      "_model_module_version": "~0.2.2",
      "_model_name": "InnotaterModel",
      "_view_count": null,
      "_view_module": "jupyter-innotater",
      "_view_module_version": "~0.2.2",
      "_view_name": "InnotaterView",
      "box_style": "",
      "children": [
       "IPY_MODEL_bca4be32bcf84dc3bad36014cec87753",
       "IPY_MODEL_c49643bff7bc4ba2a177e837d4616a87"
      ],
      "index": 0,
      "is_dirty": false,
      "keyboard_shortcuts": true,
      "layout": "IPY_MODEL_37fef569811440b490fefa86a1354e37"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}